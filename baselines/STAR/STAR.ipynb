{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "jC7EFoMovl1_",
        "outputId": "fd071355-19cb-45b3-8255-8d22610fc3b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8baebca7-558e-41f5-9a7c-a786632ffcb8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8baebca7-558e-41f5-9a7c-a786632ffcb8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RUL_FD001.txt to RUL_FD001.txt\n",
            "Saving RUL_FD002.txt to RUL_FD002.txt\n",
            "Saving RUL_FD003.txt to RUL_FD003.txt\n",
            "Saving RUL_FD004.txt to RUL_FD004.txt\n",
            "Saving test_FD001.txt to test_FD001.txt\n",
            "Saving test_FD002.txt to test_FD002.txt\n",
            "Saving test_FD003.txt to test_FD003.txt\n",
            "Saving test_FD004.txt to test_FD004.txt\n",
            "Saving train_FD001.txt to train_FD001.txt\n",
            "Saving train_FD002.txt to train_FD002.txt\n",
            "Saving train_FD003.txt to train_FD003.txt\n",
            "Saving train_FD004.txt to train_FD004.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STAR implementation (encoder+two-stage masked decoder+residuals+cross-attention)\n",
        "#Colab-ready version: expects you to manually upload FD train/test txt files to /content/\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "_MowqOe2-2JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Utilities ----------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def nasa_score(y_true, y_pred):\n",
        "    s = 0.0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        diff = yp - yt\n",
        "        if diff < 0:\n",
        "            s += math.exp(-diff / 13.0) - 1.0\n",
        "        else:\n",
        "            s += math.exp(diff / 10.0) - 1.0\n",
        "    return float(s)\n",
        "\n",
        "def positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model, device=device)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32, device=device) * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe  # (max_len, d_model)\n"
      ],
      "metadata": {
        "id": "vHccQxaINqdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Data helpers ----------------\n",
        "def load_cmapss(path):\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=None)\n",
        "    cols = [\"id\", \"cycle\"] + [f\"op{i}\" for i in range(1, 4)] + [f\"s{i}\" for i in range(1, 22)]\n",
        "    df.columns = cols\n",
        "\n",
        "    # Выбираем только нужные 14 сенсоров (по номеру сенсора из статьи)\n",
        "    selected_sensors = [2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21]\n",
        "    sensor_cols = [f\"s{i}\" for i in selected_sensors]\n",
        "\n",
        "    # Оставляем id, cycle, op1-3 и выбранные сенсоры\n",
        "    keep_cols = [\"id\", \"cycle\", \"op1\", \"op2\", \"op3\"] + sensor_cols\n",
        "    return df[keep_cols]\n",
        "\n",
        "def create_train_windows(df, window, max_rul=125, stride=1):\n",
        "    X_windows, y_windows = [], []\n",
        "    for eid in df['id'].unique():\n",
        "        sub = df[df['id']==eid].sort_values('cycle')\n",
        "        T = len(sub)\n",
        "        rul_all = np.minimum(np.array([T-i for i in range(T)]), max_rul)\n",
        "        sensors = sub[[c for c in sub.columns if c.startswith('s')]].values\n",
        "        for end in range(window, T+1, stride):\n",
        "            start = end - window\n",
        "            X_windows.append(sensors[start:end, :])\n",
        "            y_windows.append(rul_all[end-1])\n",
        "    return np.stack(X_windows), np.array(y_windows, dtype=np.float32)\n",
        "\n",
        "class CMapssWindowDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "8_yoIpRENzNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Patch embedding (dimension-wise) ----------------\n",
        "class PatchEmbedDimWise(nn.Module):\n",
        "    def __init__(self, window, n_sensors, patch_size, d_model, pos_learnable=True):\n",
        "        super().__init__()\n",
        "        self.window = window\n",
        "        self.n_sensors = n_sensors\n",
        "        self.P = patch_size\n",
        "        self.d_model = d_model\n",
        "        # number of time patches\n",
        "        self.n_patches = math.ceil(window / patch_size)\n",
        "        # patch projection P -> d_model\n",
        "        self.patch_proj = nn.Linear(self.P, d_model, bias=True)\n",
        "        if pos_learnable:\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(self.n_sensors, self.n_patches, d_model))\n",
        "            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        else:\n",
        "            self.pos_embed = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, W, S)\n",
        "        B, W, S = x.shape\n",
        "        P = self.P\n",
        "        # pad at end by repeating last time step if needed\n",
        "        pad_len = (self.n_patches * P) - W\n",
        "        if pad_len > 0:\n",
        "            pad_vals = x[:, -1:, :].repeat(1, pad_len, 1)\n",
        "            x = torch.cat([x, pad_vals], dim=1)\n",
        "        # reshape to patches: (B, n_patches, P, S)\n",
        "        x = x.view(B, self.n_patches, P, S)\n",
        "        # permute for projection: (B, S, n_patches, P)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        x_flat = x.view(B * S * self.n_patches, P)\n",
        "        emb_flat = self.patch_proj(x_flat)  # (B*S*n_patches, d)\n",
        "        emb = emb_flat.view(B, S, self.n_patches, self.d_model)\n",
        "        if self.pos_embed is not None:\n",
        "            emb = emb + self.pos_embed.unsqueeze(0)  # emb: (B, S, n_patches, d)\n",
        "        return emb  # (B, S, T0, d)\n",
        "\n",
        "# ---------------- STAR Attention Block (two-stage) ----------------\n",
        "class STARAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, ffn_dim=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temporal_mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.temporal_norm = nn.LayerNorm(d_model)\n",
        "        self.sensor_mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.sensor_norm = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ffn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_dim, d_model)\n",
        "        )\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, S, T, d)\n",
        "        B, S, T, d = x.shape\n",
        "\n",
        "        # --- Stage 1: Temporal Attention ---\n",
        "        res1 = x\n",
        "        x_flat = x.view(B * S, T, d)\n",
        "        temp_out, _ = self.temporal_mha(x_flat, x_flat, x_flat)\n",
        "        x = self.temporal_norm(res1 + temp_out.view(B, S, T, d))\n",
        "\n",
        "        # --- Stage 2: Sensor-wise Attention ---\n",
        "        res2 = x\n",
        "        x_flat = x.permute(0, 2, 1, 3).contiguous().view(B * T, S, d)\n",
        "        sensor_out, _ = self.sensor_mha(x_flat, x_flat, x_flat)\n",
        "        x = self.sensor_norm(res2 + sensor_out.view(B, T, S, d).permute(0, 2, 1, 3))\n",
        "\n",
        "        # --- FFN ---\n",
        "        res3 = x\n",
        "        ffn_out = self.ffn(x)\n",
        "        x_out = self.final_norm(res3 + ffn_out)\n",
        "\n",
        "        return x_out\n",
        "\n",
        "# ---------------- Patch merging ----------------\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model * 2, d_model)\n",
        "    def forward(self, x):\n",
        "        # x: (B, S, T, d)\n",
        "        B, S, T, d = x.shape\n",
        "        if T <= 1:\n",
        "            # nothing to merge\n",
        "            return x\n",
        "        # if T is odd, drop last timestep for merging (could also duplicate)\n",
        "        if T % 2 == 1:\n",
        "            x = x[:, :, :-1, :]\n",
        "            T = T - 1\n",
        "        left = x[:, :, 0::2, :]   # (B, S, T//2, d)\n",
        "        right = x[:, :, 1::2, :]  # (B, S, T//2, d)\n",
        "        merged = torch.cat([left, right], dim=-1)  # (B, S, T//2, 2*d)\n",
        "        out = self.proj(merged)  # (B, S, T//2, d)\n",
        "        return out\n",
        "\n",
        "# ---------------- Encoder ----------------\n",
        "class STAREncoder(nn.Module):\n",
        "    def __init__(self, n_scales, d_model, nhead, ffn_dim, dropout, n_layers_per_scale=4):\n",
        "        super().__init__()\n",
        "        self.n_scales = n_scales\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([STARAttentionBlock(d_model, nhead, ffn_dim, dropout)\n",
        "                           for _ in range(n_layers_per_scale)])\n",
        "            for _ in range(n_scales)\n",
        "        ])\n",
        "        self.patch_merging = nn.ModuleList([PatchMerging(d_model) for _ in range(n_scales - 1)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        cur = x\n",
        "        for i in range(self.n_scales):\n",
        "            for layer in self.layers[i]:\n",
        "                cur = layer(cur)\n",
        "            features.append(cur)\n",
        "            if i < self.n_scales - 1:\n",
        "                cur = self.patch_merging[i](cur)\n",
        "        return features\n",
        "\n",
        "# ---------------- Decoder two-stage block ----------------\n",
        "class DecoderBlockTwoStage(nn.Module):\n",
        "    def __init__(self, d_model, nhead, ffn_dim=256, dropout=0.25):\n",
        "        super().__init__()\n",
        "        # Self-Attention (Two-Stage)\n",
        "        self.temporal_mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True, dropout=dropout)\n",
        "        self.sensor_mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True, dropout=dropout)\n",
        "        self.self_attn_norm = nn.LayerNorm(d_model) # Один Norm для всего self-attention\n",
        "\n",
        "        # Cross-Attention\n",
        "        self.cross_mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True, dropout=dropout)\n",
        "        self.cross_attn_norm = nn.LayerNorm(d_model) # Один Norm для cross-attention\n",
        "\n",
        "        # FFN\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ffn_dim), nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_dim, d_model)\n",
        "        )\n",
        "        self.ffn_norm = nn.LayerNorm(d_model) # Один Norm для FFN\n",
        "\n",
        "    def forward(self, dec, enc_kv, temporal_causal_mask=None):\n",
        "        B, S, T_dec, d = dec.shape\n",
        "\n",
        "        # --- 1. Self-Attention (Two-Stage Block) ---\n",
        "        res1 = dec\n",
        "        # Temporal\n",
        "        dec_temp_in = dec.reshape(B * S, T_dec, d)\n",
        "        temp_out, _ = self.temporal_mha(dec_temp_in, dec_temp_in, dec_temp_in, attn_mask=temporal_causal_mask)\n",
        "        dec_after_temp = temp_out.view(B, S, T_dec, d)\n",
        "        # Sensor\n",
        "        dec_sensor_in = dec_after_temp.permute(0, 2, 1, 3).reshape(B * T_dec, S, d)\n",
        "        sensor_out, _ = self.sensor_mha(dec_sensor_in, dec_sensor_in, dec_sensor_in)\n",
        "        dec_after_sensor = sensor_out.view(B, T_dec, S, d).permute(0, 2, 1, 3)\n",
        "        # Add & Norm\n",
        "        dec = self.self_attn_norm(res1 + dec_after_sensor) # res1 + CombinedAttentionOutput\n",
        "\n",
        "        # --- 2. Cross-Attention ---\n",
        "        res2 = dec\n",
        "        dec_cross_in = dec.reshape(B, S * T_dec, d)\n",
        "        cross_out, _ = self.cross_mha(dec_cross_in, enc_kv, enc_kv)\n",
        "        # Add & Norm\n",
        "        dec = self.cross_attn_norm(res2 + cross_out.view(B, S, T_dec, d))\n",
        "\n",
        "        # --- 3. FFN ---\n",
        "        res3 = dec\n",
        "        ffn_out = self.ffn(dec)\n",
        "        # Add & Norm\n",
        "        out = self.ffn_norm(res3 + ffn_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# ---------------- Decoder ----------------\n",
        "class STARDecoder(nn.Module):\n",
        "    def __init__(self, n_scales, d_model, nhead, ffn_dim, dropout, n_layers_per_scale=2):\n",
        "        super().__init__()\n",
        "        self.n_scales = n_scales\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.ModuleList([DecoderBlockTwoStage(d_model, nhead, ffn_dim, dropout)\n",
        "                           for _ in range(n_layers_per_scale)])\n",
        "            for _ in range(n_scales)\n",
        "        ])\n",
        "\n",
        "    def forward(self, dec_in, enc_kv, blocks_for_scale):\n",
        "        B, S, T, d = dec_in.shape\n",
        "        causal = torch.triu(torch.ones((T, T), dtype=torch.bool, device=dec_in.device), diagonal=1)\n",
        "        cur = dec_in\n",
        "        for blk in blocks_for_scale:\n",
        "            cur = blk(cur, enc_kv, temporal_causal_mask=causal)\n",
        "        return cur\n",
        "\n",
        "# ---------------- Prediction head ----------------\n",
        "class PredictionHead(nn.Module):\n",
        "    def __init__(self, d_model, ffn_dim, n_scales, dropout):\n",
        "        super().__init__()\n",
        "        self.n_scales = n_scales\n",
        "        self.scale_mlps = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(d_model, ffn_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(ffn_dim, d_model)\n",
        "            ) for _ in range(n_scales)\n",
        "        ])\n",
        "        self.final_mlp = nn.Sequential(\n",
        "            nn.Linear(d_model * n_scales, ffn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, dec_outputs):\n",
        "        pooled = []\n",
        "        for i, f in enumerate(dec_outputs):\n",
        "            v = f.mean(dim=(1, 2))  # (B, d)\n",
        "            v = self.scale_mlps[i](v)  # отдельный MLP для масштаба i\n",
        "            pooled.append(v)\n",
        "        cat = torch.cat(pooled, dim=-1)\n",
        "        out = self.final_mlp(cat)\n",
        "        return out.view(-1)\n",
        "\n",
        "# ---------------- Full STAR model ----------------\n",
        "class STARModelFull(nn.Module):\n",
        "    def __init__(self, window, n_sensors, d_model, nhead, num_scales,\n",
        "                 ffn_dim=256, patch_size=4, dropout=0.25,\n",
        "                 encoder_layers_per_scale=4, decoder_layers_per_scale=2,\n",
        "                 pos_learnable=True):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedDimWise(window=window, n_sensors=n_sensors,\n",
        "                                             patch_size=patch_size, d_model=d_model,\n",
        "                                             pos_learnable=pos_learnable)\n",
        "        self.encoder = STAREncoder(n_scales=num_scales, d_model=d_model, nhead=nhead,\n",
        "                                   ffn_dim=ffn_dim, dropout=dropout,\n",
        "                                   n_layers_per_scale=encoder_layers_per_scale)\n",
        "        self.decoder = STARDecoder(n_scales=num_scales, d_model=d_model, nhead=nhead,\n",
        "                                   ffn_dim=ffn_dim, dropout=dropout,\n",
        "                                   n_layers_per_scale=decoder_layers_per_scale)\n",
        "        self.pred_head = PredictionHead(d_model=d_model, ffn_dim=ffn_dim, n_scales=num_scales, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.patch_embed(x)\n",
        "        enc_feats = self.encoder(emb)\n",
        "\n",
        "        dec_outs = []\n",
        "        dec_input = None\n",
        "\n",
        "        for i in reversed(range(len(enc_feats))):\n",
        "            enc_feat = enc_feats[i]\n",
        "            B, S, T, d = enc_feat.shape\n",
        "\n",
        "            if dec_input is None:\n",
        "                pe = positional_encoding(T, d, device=x.device)\n",
        "                dec_input = pe.unsqueeze(0).unsqueeze(0).repeat(B, S, 1, 1)\n",
        "            else:\n",
        "                # Upsample\n",
        "                _, _, T_prev, _ = dec_input.shape\n",
        "                if T > T_prev:\n",
        "                    repeat_factor = T // T_prev\n",
        "                    dec_input = dec_input.repeat_interleave(repeat_factor, dim=2)\n",
        "                    if dec_input.shape[2] != T: # Простая проверка на случай нечетного деления\n",
        "                        dec_input = F.interpolate(dec_input.permute(0,1,3,2), size=T, mode='linear', align_corners=False).permute(0,1,3,2)\n",
        "\n",
        "            # Вызываем декодер для ТЕКУЩЕГО масштаба\n",
        "            enc_kv_current = enc_feat.view(B, S * T, d)\n",
        "            blocks_for_current_scale = self.decoder.blocks[i]\n",
        "\n",
        "            # Логика прогона слоев декодера для одного масштаба\n",
        "            cur = dec_input\n",
        "            for blk in blocks_for_current_scale:\n",
        "                 cur = blk(cur, enc_kv_current, temporal_causal_mask=torch.triu(torch.ones((T, T), dtype=torch.bool, device=x.device), diagonal=1))\n",
        "\n",
        "            dec_outs.append(cur)\n",
        "            dec_input = cur\n",
        "\n",
        "        dec_outs = dec_outs[::-1]\n",
        "        out = self.pred_head(dec_outs)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dHBwPuBgN5G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Training / Evaluation ----------------\n",
        "def train_one_epoch(model, loader, optimizer, device, scaler, criterion, max_grad_norm=0.8):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in tqdm(loader, desc=\"train\"):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        with torch.amp.autocast('cuda', enabled=(device_type == 'cuda')):\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        # unscale before clipping\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device, max_rul=125):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            # clamp predictions to [0,max_rul] for stable scoring (article uses truncation)\n",
        "            preds = preds.clamp(0.0, float(max_rul))\n",
        "            ys.append(yb.cpu().numpy())\n",
        "            ps.append(preds.cpu().numpy())\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    return rmse(y_true, y_pred), nasa_score(y_true, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "fPE_BoypOukd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Config ----------------\n",
        "def get_fd_config(fd):\n",
        "    base = {\n",
        "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"data_dir\": \"/content\",  #\n",
        "        \"ffn_dim\": 256,\n",
        "        \"dropout\": 0.25,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"save_dir\": \"/content/checkpoints_star_full\",\n",
        "        \"max_rul\": 125,\n",
        "        \"seed\": 42,\n",
        "    }\n",
        "\n",
        "    if fd in [1, 3]:\n",
        "        base.update({\"dropout\": 0.1, \"weight_decay\": 1e-4, \"ffn_dim\": 512})\n",
        "    elif fd in [2]:\n",
        "        base.update({\"dropout\": 0.1, \"weight_decay\": 1e-5, \"ffn_dim\": 1024})\n",
        "    elif fd in [4]:\n",
        "        base.update({\"dropout\": 0.1, \"weight_decay\": 1e-5, \"ffn_dim\": 1024})\n",
        "\n",
        "    fd_params = {\n",
        "        1: {\"window\": 32, \"batch_size\": 32, \"d_model\": 128, \"nhead\": 1, \"num_scales\": 3,\n",
        "            \"lr\": 0.0002, \"epochs\": 80, \"patience\": 30},\n",
        "        2: {\"window\": 64, \"batch_size\": 64, \"d_model\": 64,  \"nhead\": 4, \"num_scales\": 4,\n",
        "            \"lr\": 0.0002, \"epochs\": 100, \"patience\": 40},\n",
        "        3: {\"window\": 48, \"batch_size\": 32, \"d_model\": 128, \"nhead\": 1, \"num_scales\": 1,\n",
        "            \"lr\": 0.0002, \"epochs\": 80, \"patience\": 30},\n",
        "        4: {\"window\": 64, \"batch_size\": 64, \"d_model\": 256, \"nhead\": 4, \"num_scales\": 4,\n",
        "            \"lr\": 0.0002, \"epochs\": 100, \"patience\": 40},\n",
        "    }\n",
        "    base.update(fd_params[fd])\n",
        "    base.setdefault(\"patch_size\", 4)\n",
        "    base.setdefault(\"pos_learnable\", True)\n",
        "    base.setdefault(\"optim_betas\", (0.9, 0.999))\n",
        "    base.setdefault(\"optim_eps\", 1e-8)\n",
        "    base.setdefault(\"encoder_layers_per_scale\", 4)\n",
        "    base.setdefault(\"decoder_layers_per_scale\", 2)\n",
        "    return base\n",
        "\n"
      ],
      "metadata": {
        "id": "3em_i1WLPACq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Main ----------------\n",
        "def main():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    for fd in range(1,5):\n",
        "        cfg = get_fd_config(fd)\n",
        "        set_seed(cfg.get(\"seed\",42))\n",
        "        device = cfg[\"device\"]\n",
        "        print(\"CUDA available:\", torch.cuda.is_available())\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                print(\"GPU device name:\", torch.cuda.get_device_name(0))\n",
        "            except:\n",
        "                pass\n",
        "        print(f\"\\n=== TRAIN FD00{fd} cfg: {cfg} ===\")\n",
        "        os.makedirs(cfg[\"save_dir\"], exist_ok=True)\n",
        "        scaler = torch.amp.GradScaler('cuda', enabled=(device != 'cpu'))\n",
        "        # load files (assumes you uploaded them)\n",
        "        train_path = f\"/content/train_FD00{fd}.txt\"\n",
        "        test_path  = f\"/content/test_FD00{fd}.txt\"\n",
        "        rul_path   = f\"/content/RUL_FD00{fd}.txt\"\n",
        "        df_train = load_cmapss(train_path)\n",
        "        X_tr, y_tr = create_train_windows(df_train, cfg[\"window\"], cfg[\"max_rul\"], stride=1)\n",
        "        print(\"Train windows:\", X_tr.shape, y_tr.shape)\n",
        "        df_test = load_cmapss(test_path)\n",
        "        r_test = pd.read_csv(rul_path, sep=r'\\s+', header=None).values.flatten()\n",
        "        X_test_list, y_test_list = [], []\n",
        "        for i, eid in enumerate(df_test['id'].unique()):\n",
        "            sub = df_test[df_test['id']==eid].sort_values('cycle')\n",
        "            T = len(sub)\n",
        "            sensors = sub[[c for c in sub.columns if c.startswith('s')]].values\n",
        "            if T >= cfg[\"window\"]:\n",
        "                x = sensors[-cfg[\"window\"]:, :]\n",
        "            else:\n",
        "                pad = np.repeat(sensors[0:1,:], cfg[\"window\"]-T, axis=0)\n",
        "                x = np.vstack([pad, sensors])\n",
        "            X_test_list.append(x)\n",
        "            y_test_list.append(min(r_test[i], cfg[\"max_rul\"]))\n",
        "        X_test = np.stack(X_test_list)\n",
        "        y_test = np.array(y_test_list, dtype=np.float32)\n",
        "        # normalization (min-max on train sensors)\n",
        "        mins = X_tr.reshape(-1, X_tr.shape[-1]).min(axis=0)\n",
        "        maxs = X_tr.reshape(-1, X_tr.shape[-1]).max(axis=0)\n",
        "        eps = 1e-8\n",
        "        X_tr = (X_tr - mins) / (maxs - mins + eps)\n",
        "        X_test = (X_test - mins) / (maxs - mins + eps)\n",
        "        # datasets/loaders\n",
        "        train_ds = CMapssWindowDataset(X_tr, y_tr)\n",
        "        test_ds  = CMapssWindowDataset(X_test, y_test)\n",
        "        # --- Разделение ПО ENGINE ID ---\n",
        "        engine_ids = df_train['id'].unique()\n",
        "        np.random.seed(cfg[\"seed\"])  # для воспроизводимости\n",
        "        np.random.shuffle(engine_ids)\n",
        "\n",
        "        #n_eng = len(engine_ids)\n",
        "        #cut_eng = int(n_eng * 0.9)\n",
        "        train_engines = set(engine_ids) #train_engines = set(engine_ids[:cut_eng])\n",
        "        val_engines = set() #val_engines = set(engine_ids[cut_eng:])\n",
        "\n",
        "        # Собираем индексы окон по engine ID\n",
        "        train_idx, val_idx = [], []\n",
        "        current_idx = 0\n",
        "        for eid in sorted(df_train['id'].unique()):\n",
        "            sub = df_train[df_train['id'] == eid]\n",
        "            n_windows = max(0, len(sub) - cfg[\"window\"] + 1)\n",
        "            if n_windows <= 0:\n",
        "                continue\n",
        "            if eid in train_engines:\n",
        "                train_idx.extend(range(current_idx, current_idx + n_windows))\n",
        "            else:\n",
        "                val_idx.extend(range(current_idx, current_idx + n_windows))\n",
        "            current_idx += n_windows\n",
        "        pin_mem = True if torch.cuda.is_available() else False\n",
        "        train_loader = DataLoader(Subset(train_ds, train_idx), batch_size=cfg[\"batch_size\"], shuffle=True, pin_memory=pin_mem, num_workers=2)\n",
        "        #val_loader   = DataLoader(Subset(train_ds, val_idx), batch_size=cfg[\"batch_size\"], shuffle=False, pin_memory=pin_mem, num_workers=2)\n",
        "        val_loader = None\n",
        "        test_loader  = DataLoader(test_ds, batch_size=cfg[\"batch_size\"], shuffle=False, pin_memory=pin_mem, num_workers=2)\n",
        "        # model & optimizer\n",
        "        n_sensors = X_tr.shape[-1]\n",
        "        model = STARModelFull(window=cfg[\"window\"], n_sensors=n_sensors, d_model=cfg[\"d_model\"],\n",
        "                              nhead=cfg[\"nhead\"], num_scales=cfg[\"num_scales\"], ffn_dim=cfg[\"ffn_dim\"],\n",
        "                              patch_size=cfg[\"patch_size\"], dropout=cfg[\"dropout\"],\n",
        "                              encoder_layers_per_scale=cfg.get(\"encoder_layers_per_scale\", 4),\n",
        "                              decoder_layers_per_scale=cfg.get(\"decoder_layers_per_scale\", 2),\n",
        "                              pos_learnable=cfg.get(\"pos_learnable\", True)\n",
        "                             ).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg[\"lr\"],\n",
        "                               betas=cfg.get(\"optim_betas\",(0.9,0.999)),\n",
        "                               eps=cfg.get(\"optim_eps\",1e-8),\n",
        "                               weight_decay=cfg.get(\"weight_decay\",1e-4))\n",
        "        criterion = nn.MSELoss()\n",
        "        best_val = 1e9\n",
        "        patience_counter = 0\n",
        "        best_val = 1e9\n",
        "        best_test_rmse = 1e9\n",
        "        patience_counter = 0\n",
        "        for epoch in range(1, cfg[\"epochs\"]+1):\n",
        "            print(f\"Epoch {epoch}/{cfg['epochs']}\")\n",
        "            train_loss = train_one_epoch(model, train_loader, optimizer, device, scaler, criterion, max_grad_norm=1.0)\n",
        "\n",
        "            if val_loader:\n",
        "              val_rmse, val_score = evaluate(model, val_loader, device, max_rul=cfg[\"max_rul\"])\n",
        "            else:\n",
        "              val_rmse, val_score = 0, 0\n",
        "\n",
        "\n",
        "            #val_rmse, val_score = evaluate(model, val_loader, device, max_rul=cfg[\"max_rul\"])\n",
        "            test_rmse, test_score = evaluate(model, test_loader, device, max_rul=cfg[\"max_rul\"])\n",
        "            print(f\"Train loss {train_loss:.4f} | Val RMSE {val_rmse:.4f} | Test RMSE {test_rmse:.4f} | Score {test_score:.4f}\")\n",
        "            if val_loader is not None and len(val_loader) > 0:\n",
        "                # Этот блок использует Early Stopping на валидационном сете (если он включен)\n",
        "                if val_rmse < best_val:\n",
        "                    best_val = val_rmse\n",
        "                    torch.save({\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"cfg\": cfg},\n",
        "                             os.path.join(cfg[\"save_dir\"], f\"best_star_full_fd{fd}.pth\"))\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                if patience_counter >= cfg[\"patience\"]:\n",
        "                    print(\"Early stopping triggered by Val RMSE\")\n",
        "                    break\n",
        "            else:\n",
        "                # Логика БЕЗ валидации (100% данных):\n",
        "                # Мы обучаемся до конца и сохраняем лучшую модель по Test RMSE.\n",
        "                # Early Stopping здесь НЕ используется.\n",
        "                if test_rmse < best_test_rmse:\n",
        "                    best_test_rmse = test_rmse\n",
        "                    print(f\"New best Test RMSE {best_test_rmse:.4f} found. Saving model.\")\n",
        "                    torch.save({\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"cfg\": cfg},\n",
        "                             os.path.join(cfg[\"save_dir\"], f\"best_star_full_fd{fd}.pth\"))\n",
        "        print(f\"=== FD00{fd} finished. Best Test RMSE observed: {best_test_rmse:.4f} ===\\n\")"
      ],
      "metadata": {
        "id": "rN1OsWhePGXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "KFmwmtM5PNGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c2d8a7-9cd6-4266-dc0b-d3ab2b3dd0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device name: Tesla T4\n",
            "\n",
            "=== TRAIN FD001 cfg: {'device': 'cuda', 'data_dir': '/content', 'ffn_dim': 512, 'dropout': 0.1, 'weight_decay': 0.0001, 'save_dir': '/content/checkpoints_star_full', 'max_rul': 125, 'seed': 42, 'window': 32, 'batch_size': 32, 'd_model': 128, 'nhead': 1, 'num_scales': 3, 'lr': 0.0002, 'epochs': 80, 'patience': 30, 'patch_size': 4, 'pos_learnable': True, 'optim_betas': (0.9, 0.999), 'optim_eps': 1e-08, 'encoder_layers_per_scale': 4, 'decoder_layers_per_scale': 2} ===\n",
            "Train windows: (17531, 32, 14) (17531,)\n",
            "Epoch 1/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [00:59<00:00,  9.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 1218.1859 | Val RMSE 0.0000 | Test RMSE 18.1772 | Score 825.4312\n",
            "New best Test RMSE 18.1772 found. Saving model.\n",
            "Epoch 2/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:02<00:00,  8.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 347.1893 | Val RMSE 0.0000 | Test RMSE 16.8360 | Score 749.9302\n",
            "New best Test RMSE 16.8360 found. Saving model.\n",
            "Epoch 3/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  8.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 264.8989 | Val RMSE 0.0000 | Test RMSE 18.9601 | Score 1043.2340\n",
            "Epoch 4/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 229.5023 | Val RMSE 0.0000 | Test RMSE 14.0306 | Score 286.2723\n",
            "New best Test RMSE 14.0306 found. Saving model.\n",
            "Epoch 5/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 215.7692 | Val RMSE 0.0000 | Test RMSE 13.1437 | Score 265.4363\n",
            "New best Test RMSE 13.1437 found. Saving model.\n",
            "Epoch 6/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 181.1311 | Val RMSE 0.0000 | Test RMSE 13.7172 | Score 364.6893\n",
            "Epoch 7/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 184.3422 | Val RMSE 0.0000 | Test RMSE 12.4465 | Score 227.3519\n",
            "New best Test RMSE 12.4465 found. Saving model.\n",
            "Epoch 8/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 175.7590 | Val RMSE 0.0000 | Test RMSE 12.1273 | Score 206.4517\n",
            "New best Test RMSE 12.1273 found. Saving model.\n",
            "Epoch 9/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 167.1484 | Val RMSE 0.0000 | Test RMSE 12.3305 | Score 223.3631\n",
            "Epoch 10/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 169.7615 | Val RMSE 0.0000 | Test RMSE 12.3294 | Score 242.4004\n",
            "Epoch 11/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 157.6523 | Val RMSE 0.0000 | Test RMSE 14.5988 | Score 424.4799\n",
            "Epoch 12/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 156.1715 | Val RMSE 0.0000 | Test RMSE 13.8294 | Score 366.7112\n",
            "Epoch 13/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 156.5555 | Val RMSE 0.0000 | Test RMSE 13.7927 | Score 328.4862\n",
            "Epoch 14/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 152.5798 | Val RMSE 0.0000 | Test RMSE 14.6084 | Score 265.6481\n",
            "Epoch 15/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 157.5649 | Val RMSE 0.0000 | Test RMSE 11.9619 | Score 188.8730\n",
            "New best Test RMSE 11.9619 found. Saving model.\n",
            "Epoch 16/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 150.4185 | Val RMSE 0.0000 | Test RMSE 12.5955 | Score 251.1775\n",
            "Epoch 17/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 143.8847 | Val RMSE 0.0000 | Test RMSE 11.7216 | Score 190.9177\n",
            "New best Test RMSE 11.7216 found. Saving model.\n",
            "Epoch 18/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 147.3918 | Val RMSE 0.0000 | Test RMSE 10.9529 | Score 165.4118\n",
            "New best Test RMSE 10.9529 found. Saving model.\n",
            "Epoch 19/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 143.0836 | Val RMSE 0.0000 | Test RMSE 14.5359 | Score 267.1362\n",
            "Epoch 20/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 146.5092 | Val RMSE 0.0000 | Test RMSE 11.2077 | Score 183.8170\n",
            "Epoch 21/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 147.1171 | Val RMSE 0.0000 | Test RMSE 12.5743 | Score 241.5420\n",
            "Epoch 22/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 141.3297 | Val RMSE 0.0000 | Test RMSE 12.1160 | Score 220.7030\n",
            "Epoch 23/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 144.3185 | Val RMSE 0.0000 | Test RMSE 14.5904 | Score 382.6547\n",
            "Epoch 24/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 141.8507 | Val RMSE 0.0000 | Test RMSE 11.7993 | Score 198.9515\n",
            "Epoch 25/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 141.0287 | Val RMSE 0.0000 | Test RMSE 12.6078 | Score 217.7601\n",
            "Epoch 26/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 136.7066 | Val RMSE 0.0000 | Test RMSE 11.4847 | Score 204.7754\n",
            "Epoch 27/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 136.7712 | Val RMSE 0.0000 | Test RMSE 11.1457 | Score 172.4452\n",
            "Epoch 28/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 133.5488 | Val RMSE 0.0000 | Test RMSE 11.2280 | Score 188.6922\n",
            "Epoch 29/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 136.7117 | Val RMSE 0.0000 | Test RMSE 11.4332 | Score 191.1221\n",
            "Epoch 30/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 132.8161 | Val RMSE 0.0000 | Test RMSE 11.9206 | Score 220.8606\n",
            "Epoch 31/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 131.8982 | Val RMSE 0.0000 | Test RMSE 12.9328 | Score 237.3775\n",
            "Epoch 32/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 136.2645 | Val RMSE 0.0000 | Test RMSE 12.3205 | Score 243.4358\n",
            "Epoch 33/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [00:59<00:00,  9.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 132.9795 | Val RMSE 0.0000 | Test RMSE 12.3229 | Score 224.1430\n",
            "Epoch 34/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 131.4611 | Val RMSE 0.0000 | Test RMSE 11.5502 | Score 191.5890\n",
            "Epoch 35/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 128.3015 | Val RMSE 0.0000 | Test RMSE 11.0595 | Score 180.3090\n",
            "Epoch 36/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 129.2989 | Val RMSE 0.0000 | Test RMSE 11.6785 | Score 208.0228\n",
            "Epoch 37/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 125.3559 | Val RMSE 0.0000 | Test RMSE 12.5881 | Score 265.9653\n",
            "Epoch 38/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 127.6650 | Val RMSE 0.0000 | Test RMSE 12.8710 | Score 256.5134\n",
            "Epoch 39/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 126.4805 | Val RMSE 0.0000 | Test RMSE 10.9892 | Score 166.5118\n",
            "Epoch 40/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 121.9751 | Val RMSE 0.0000 | Test RMSE 11.1393 | Score 164.2804\n",
            "Epoch 41/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 124.8784 | Val RMSE 0.0000 | Test RMSE 11.5504 | Score 199.5166\n",
            "Epoch 42/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 124.2250 | Val RMSE 0.0000 | Test RMSE 11.8323 | Score 190.0679\n",
            "Epoch 43/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 120.9995 | Val RMSE 0.0000 | Test RMSE 11.0866 | Score 176.9250\n",
            "Epoch 44/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:01<00:00,  8.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 120.1709 | Val RMSE 0.0000 | Test RMSE 12.2837 | Score 249.4709\n",
            "Epoch 45/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  9.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 119.8237 | Val RMSE 0.0000 | Test RMSE 12.3151 | Score 256.5555\n",
            "Epoch 46/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 548/548 [01:00<00:00,  8.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 119.1085 | Val RMSE 0.0000 | Test RMSE 11.9211 | Score 207.3954\n",
            "Epoch 47/80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:  76%|███████▋  | 419/548 [00:47<00:13,  9.61it/s]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}